{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 22\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msequence\u001b[39;00m \u001b[39mimport\u001b[39;00m pad_sequences\n\u001b[1;32m     21\u001b[0m model\u001b[39m=\u001b[39m Sequential()\n\u001b[0;32m---> 22\u001b[0m model\u001b[39m.\u001b[39madd(Embedding(vocab_size, embedding_size, input_length\u001b[39m=\u001b[39mmax_len))\n\u001b[1;32m     23\u001b[0m model\u001b[39m.\u001b[39madd(Conv1D(filters\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m, kernel_size\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msame\u001b[39m\u001b[39m'\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m     24\u001b[0m model\u001b[39m.\u001b[39madd(MaxPooling1D(pool_size\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vocab_size' is not defined"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense, Dropout\n",
    "from keras.metrics import Precision, Recall\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import datasets\n",
    "import pickle\n",
    "\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.callbacks import History\n",
    "\n",
    "from keras import losses\n",
    "import warnings\n",
    "import warnings\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model= Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_size, input_length=max_len))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Bidirectional(LSTM(32)))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.load_weights(\"/workspace/project_new/dl models/best_model predefined.h5\")\n",
    "tokenizer = pickle.load(open('dl models/tokenizer_predefined.pickle', 'rb'))\n",
    "\n",
    "X = tokenizer.texts_to_matrix(\"this is a tweeet\")\n",
    "\n",
    "padded_text = pad_sequences(X, padding='post', maxlen=max_len)\n",
    "health_class = ['Reliable', 'Unreliable']\n",
    "health_class[model.predict(padded_text).argmax(axis = 1)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dl_model(model_path,tokenizer_path): \n",
    "    vocab_size = 5000\n",
    "    embedding_size = 32\n",
    "    epochs=10\n",
    "    max_len=50\n",
    "    model= Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_size, input_length=max_len))\n",
    "    model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Bidirectional(LSTM(32)))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.load_weights(model_path)\n",
    "    tokenizer = pickle.load(open(tokenizer_path, 'rb'))\n",
    "    return model , tokenizer\n",
    "\n",
    "\n",
    "def predict_classes(text): \n",
    "    model, tokenizer = load_dl_model(\"dl models/best_model predefined.h5\",\"dl models/tokenizer_predefined.pickle\" )\n",
    "    tokenized_text = tokenizer.texts_to_matrix(text)\n",
    "    padded_text = pad_sequences(tokenized_text,padding='post', maxlen=50)\n",
    "    health_class = ['Reliable', 'Unreliable']\n",
    "    prediction = health_class[model.predict(padded_text).argmax(axis = 1)[0]]\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "import tf_geometric as tfg\n",
    "num_classes =6\n",
    "from tensorflow import keras\n",
    "import pickle\n",
    "\n",
    "\n",
    "class GCNModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.gcn0 = tfg.layers.GCN(100, activation=tf.nn.relu)\n",
    "        self.gcn1 = tfg.layers.GCN(100, activation=tf.nn.relu)\n",
    "        self.gcn2 = tfg.layers.GCN(50, activation=tf.nn.relu)\n",
    "        self.gcn3 = tfg.layers.GCN(num_classes)\n",
    "        self.dropout = keras.layers.Dropout(0.5)\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None, cache=None):\n",
    "        x, edge_index, edge_weight = inputs\n",
    "        h = self.gcn0([x, edge_index, edge_weight], cache=cache)\n",
    "        h = self.gcn1([h, edge_index, edge_weight], cache=cache)\n",
    "        h = self.dropout(h, training=training)\n",
    "        h = self.gcn2([h, edge_index, edge_weight], cache=cache)\n",
    "        h = self.gcn3([h, edge_index, edge_weight], cache=cache)\n",
    "        return h\n",
    "def load_gnn_model(tokenizer_path, pmi_model_path): \n",
    "    tokenizer = pickle.load(open(\"gnn_tokenizer.pkl\", 'rb'))\n",
    "    pmi_model_graph = pickle.load(open(\"GNN model/cached_pmi_model.p\", \"rb\"))  \n",
    "    embedding_size = 150\n",
    "    num_words = len(tokenizer.word_index) + 1\n",
    "    test_graph = build_word_graph(num_words,pmi_model_graph  , embedding_size)\n",
    "    return tokenizer , test_graph\n",
    "\n",
    "\n",
    "gnn_model = GCNModel()\n",
    "class PMIModel(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.word_counter = None\n",
    "        self.pair_counter = None\n",
    "\n",
    "    def get_pair_id(self, word0, word1):\n",
    "        pair_id = tuple(sorted([word0, word1]))\n",
    "        return pair_id\n",
    "\n",
    "    def fit(self, sequences, window_size):\n",
    "\n",
    "        self.word_counter = Counter()\n",
    "        self.pair_counter = Counter()\n",
    "        num_windows = 0\n",
    "        for sequence in tqdm(sequences):\n",
    "            for offset in range(len(sequence) - window_size):\n",
    "                window = sequence[offset:offset + window_size]\n",
    "                num_windows += 1\n",
    "                for i, word0 in enumerate(window):\n",
    "                    self.word_counter[word0] += 1\n",
    "                    for j, word1 in enumerate(window[i + 1:]):\n",
    "                        pair_id = self.get_pair_id(word0, word1)\n",
    "                        self.pair_counter[pair_id] += 1\n",
    "\n",
    "        for word, count in self.word_counter.items():\n",
    "            self.word_counter[word] = count / num_windows\n",
    "        for pair_id, count in self.pair_counter.items():\n",
    "            self.pair_counter[pair_id] = count / num_windows\n",
    "\n",
    "    def transform(self, word0, word1):\n",
    "        prob_a = self.word_counter[word0]\n",
    "        prob_b = self.word_counter[word1]\n",
    "        pair_id = self.get_pair_id(word0, word1)\n",
    "        prob_pair = self.pair_counter[pair_id]\n",
    "        if prob_a == 0 or prob_b == 0 or prob_pair == 0:\n",
    "           return 0\n",
    "\n",
    "        pmi = np.log(prob_pair / (prob_a * prob_b))\n",
    "        # print(word0, word1, pmi)\n",
    "        pmi = np.maximum(pmi, 0.0)\n",
    "        # print(pmi)\n",
    "        return pmi\n",
    "graph = pickle.load(open(\"GNN model/cached_pmi_model.p\", \"rb\"))\n",
    "tokenizer = pickle.load(open(\"gnn_tokenizer.pkl\", 'rb'))\n",
    "tokenized_text = tokenizer.texts_to_matrix(\"this is a tweet\")\n",
    "padded_text = pad_sequences(tokenized_text)\n",
    "def build_combined_graph(word_graph, sequences, embedding_size):\n",
    "    num_words = word_graph.num_nodes\n",
    "    x = tf.zeros([len(sequences), embedding_size], dtype=tf.float32)\n",
    "    edges = []\n",
    "    edge_weight = []\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        doc_node_index = num_words + i\n",
    "        for word in sequence:\n",
    "            edges.append([doc_node_index, word])  # only directed edge\n",
    "            edge_weight.append(1.0)  # use BOW instaead of TF-IDF\n",
    "\n",
    "    edge_index = np.array(edges).T\n",
    "    x = tf.concat([word_graph.x, x], axis=0)\n",
    "    edge_index = np.concatenate([word_graph.edge_index, edge_index], axis=1)\n",
    "    edge_weight = np.concatenate([word_graph.edge_weight, edge_weight], axis=0)\n",
    "    return tfg.Graph(x=x, edge_index=edge_index, edge_weight=edge_weight)\n",
    "def build_word_graph(num_words, pmi_model, embedding_size):\n",
    "    x = tf.Variable(tf.random.truncated_normal([num_words, embedding_size], stddev=1 / np.sqrt(embedding_size)),\n",
    "                    dtype=tf.float32)\n",
    "    edges = []\n",
    "    edge_weight = []\n",
    "    for (word0, word1) in pmi_model.pair_counter.keys():\n",
    "        pmi = pmi_model.transform(word0, word1)\n",
    "        if pmi > 0:\n",
    "            edges.append([word0, word1])\n",
    "            edge_weight.append(pmi)\n",
    "            edges.append([word1, word0])\n",
    "            edge_weight.append(pmi)\n",
    "    edge_index = np.array(edges).T\n",
    "    return tfg.Graph(x=x, edge_index=edge_index, edge_weight=edge_weight)\n",
    "def predict_classes(tweet, tokenizer_path, pmi_path): \n",
    "    tokenizer , test_graph = load_gnn_model(tokenizer_path, pmi_path)\n",
    "    tokenized_text = tokenizer.texts_to_sequences([\"this is a tweet\"])\n",
    "    embedding_size = 150\n",
    "    num_words = len(tokenizer.word_index) + 1\n",
    "    logits = model([test_graph.x, test_graph.edge_index, test_graph.edge_weight],  training=False)\n",
    "    output = 0 #tf.argmax(logits[2][num_words:], axis = 0)\n",
    "    return  output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs=[<tf.Variable 'Variable:0' shape=(13184, 150) dtype=float32, numpy=\n",
      "array([[-0.00689482,  0.1378565 ,  0.0760317 , ...,  0.02666909,\n",
      "         0.06896003,  0.09562276],\n",
      "       [-0.12457988, -0.01602609, -0.02115135, ...,  0.03765443,\n",
      "         0.00964285, -0.02111768],\n",
      "       [-0.01364579,  0.05218197,  0.02022368, ...,  0.06429137,\n",
      "         0.0205862 , -0.00963278],\n",
      "       ...,\n",
      "       [ 0.01887982,  0.04620624, -0.02770656, ..., -0.02550627,\n",
      "         0.086329  , -0.03052771],\n",
      "       [-0.10828976, -0.05045774,  0.03408786, ..., -0.03381453,\n",
      "        -0.08212626, -0.03146039],\n",
      "       [ 0.06616103, -0.01973128,  0.14076221, ...,  0.04357162,\n",
      "         0.05841164,  0.07551474]], dtype=float32)>, <tf.Tensor: shape=(2, 396160), dtype=int32, numpy=\n",
      "array([[ 175,  187,  175, ..., 2819,  150, 3136],\n",
      "       [ 187,  175, 3749, ...,  150, 3136,  150]], dtype=int32)>, <tf.Tensor: shape=(396160,), dtype=float32, numpy=\n",
      "array([4.8247094, 4.8247094, 2.9720411, ..., 2.6492677, 3.4114077,\n",
      "       3.4114077], dtype=float32)>]. Consider rewriting this model with the Functional API.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "186"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_classes(\"thisasdfk  flasj dfklajsd f\" ,\"gnn_tokenizer.pkl\", \"GNN model/cached_pmi_model.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
